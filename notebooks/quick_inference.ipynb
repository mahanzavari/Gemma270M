{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0efb576",
   "metadata": {},
   "source": [
    "Quick Inference with Fine-Tuned Persian QA Model\n",
    "\n",
    "This notebook demonstrates how to load the LoRA fine-tuned Gemma-3 270M model and perform question-answering inference on new examples.\n",
    "\n",
    "We will cover the following steps:\n",
    "1.  **Setup**: Import libraries and define model paths.\n",
    "2.  **Load Model & Tokenizer**: Load the base model with 4-bit quantization and apply the trained LoRA adapters.\n",
    "3.  **Inference Function**: Create a helper function to format the prompt, generate an answer, and decode it.\n",
    "4.  **Run Examples**: Test the model with sample questions and contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286a5d7",
   "metadata": {},
   "source": [
    "# 1. Setup Configuration\n",
    "\n",
    "Before running, ensure the following paths are correct:\n",
    "- `BASE_MODEL_NAME`: The Hugging Face identifier for the base model we fine-tuned.\n",
    "- `ADAPTER_PATH`: The path to the directory where your trained LoRA adapters are saved (e.g., the `final_checkpoint` from the training script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "BASE_MODEL_NAME = \"google/gemma-3-270m\"\n",
    "# Adjust this path to point to your saved LoRA adapters\n",
    "ADAPTER_PATH = \"../outputs/final_checkpoint\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4f468",
   "metadata": {},
   "source": [
    "# 2. Load Model and Tokenizer\n",
    "\n",
    "Here, we load the base model using 4-bit quantization to reduce memory usage, which is ideal for inference on consumer GPUs. Then, we apply the fine-tuned LoRA adapters from our training process on top of it.\n",
    "\n",
    "Finally, we call `merge_and_unload()` to combine the adapter weights with the base model weights. This creates a standard transformer model in memory, which slightly increases memory usage but significantly speeds up inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13625f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Base Model with Quantization ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "print(f\"Loading base model: {BASE_MODEL_NAME}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": device.index} if device.type == \"cuda\" else \"auto\",\n",
    ")\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Load and Merge LoRA Adapters ---\n",
    "print(f\"Loading LoRA adapters from: {ADAPTER_PATH}\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "\n",
    "print(\"Merging adapters into the base model for faster inference...\")\n",
    "model = model.merge_and_unload()\n",
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18041f57",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "# 3. Inference Function\n",
    "\n",
    "This function takes a question and a context, formats them into the prompt template used during training, and generates a short answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2a293",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_response(question, context, model, tokenizer, device, max_new_tokens=64):\n",
    "    \"\"\"\n",
    "    Generates an answer given a question and a context.\n",
    "    \"\"\"\n",
    "    # Format the prompt exactly as it was during training\n",
    "    prompt = f\"پرسش: {question}\\nمتن: {context}\\nجواب کوتاه:\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False, # Use greedy decoding for deterministic output\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens, skipping the prompt part\n",
    "    response = tokenizer.decode(output_tokens[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac1056",
   "metadata": {},
   "source": [
    "# 4. Run Inference Examples\n",
    "\n",
    "Now, let's test our fine-tuned model with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8701fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example 1: Chaharshanbe Suri ---\n",
    "\n",
    "context_1 = \"\"\"\n",
    "چهارشنبه‌سوری یکی از جشن‌های ایرانی است که از غروب آخرین سه‌شنبهٔ ماه اسفند، تا پس از نیمه‌شب تا آخرین چهارشنبهٔ سال، برگزار می‌شود و برافروختن و پریدن از روی آتش مشخصهٔ اصلی آن است. این جشن، نخستین جشن از مجموعهٔ جشن‌ها و مناسبت‌های نوروزی است که با برافروختن آتش و برخی رفتارهای نمادین دیگر، به‌صورت جمعی در فضای باز برگزار می‌شود.\n",
    "\"\"\"\n",
    "question_1 = \"نام جشن آخرین سه شنبه ی سال چیست؟\"\n",
    "\n",
    "print(\"--- Example 1 ---\")\n",
    "print(f\"Context: {context_1.strip()}\")\n",
    "print(f\"Question: {question_1}\")\n",
    "\n",
    "answer_1 = generate_response(question_1, context_1, model, tokenizer, device)\n",
    "print(f\"\\nGenerated Answer: {answer_1}\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641857ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example 2: The Good, the Bad and the Ugly ---\n",
    "\n",
    "context_2 = \"\"\"\n",
    "خوب، بد، زشت یک فیلم در ژانر وسترن اسپاگتی حماسی است که توسط سرجو لئونه در سال ۱۹۶۶ در ایتالیا ساخته شد. زبانی که بازیگران این فیلم به آن تکلم می‌کنند مخلوطی از ایتالیایی و انگلیسی است. این فیلم سومین (و آخرین) فیلم از سه‌گانهٔ دلار (Dollars Trilogy) و در حال حاضر در فهرست ۲۵۰ فیلم برتر تاریخ سینمای جهان شناخته می‌شود. در فیلم، با نام «بلوندی» و «زشت» (ایلای والک، در فیلم، با نام «توکو») با هم کار می‌کنند و با شگرد خاصی، به گول زدن کلانترهای مناطق مختلف و پول درآوردن از این راه می‌پردازند. «بد» (لی وان کلیف) آدمکشی حرفه‌ای است که به خاطر پول حاضر به انجام هر کاری است.\n",
    "\"\"\"\n",
    "question_2 = \"شخصیت بد در فیلم خوب، بد، زشت چه کسی بود؟\"\n",
    "\n",
    "print(\"\\n--- Example 2 ---\")\n",
    "print(f\"Context: {context_2.strip()}\")\n",
    "print(f\"Question: {question_2}\")\n",
    "\n",
    "answer_2 = generate_response(question_2, context_2, model, tokenizer, device)\n",
    "print(f\"\\nGenerated Answer: {answer_2}\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example 3: Crescent Petroleum Contract ---\n",
    "context_3 = \"\"\"\n",
    "قرارداد کرسنت قراردادی برای فروش روزانه معادل ۵۰۰ میلیون فوت مکعب، گاز ترش میدان سلمان است، که در سال ۱۳۸۱ و در زمان وزارت بیژن نامدار زنگنه در دولت هفتم مابین شرکت کرسنت پترولیوم و شرکت ملی نفت ایران منعقد گردید. مذاکرات اولیه این قرارداد از سال ۱۹۹۷ آغاز شد و در نهایت، سال ۲۰۰۱ (۱۳۸۱) به امضای این تفاهم نامه مشترک انجامید.\n",
    "\"\"\"\n",
    "question_3 = \"قرارداد کرسنت در چه سالی منعقد شد؟\"\n",
    "\n",
    "print(\"\\n--- Example 3 ---\")\n",
    "print(f\"Context: {context_3.strip()}\")\n",
    "print(f\"Question: {question_3}\")\n",
    "\n",
    "answer_3 = generate_response(question_3, context_3, model, tokenizer, device)\n",
    "print(f\"\\nGenerated Answer: {answer_3}\")\n",
    "print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
