{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a19c0cb2",
      "metadata": {
        "id": "a19c0cb2"
      },
      "source": [
        "# Fine-tuning Gemma 3 270M on TPU v5e-8 (v5e-1) - Persian QA Dataset\n",
        "\n",
        "This notebook fine-tunes Gemma 3 270M model on TPU v5e-8 using the Persian QA translated dataset.\n",
        "\n",
        "## Features:\n",
        "- **TPU v5e-8 Support**: Optimized for Google Cloud TPU v5e-8\n",
        "- **JAX/Flax Backend**: Uses JAX for TPU acceleration\n",
        "- **Memory Efficient**: Optimized batch sizes and gradient accumulation\n",
        "- **LoRA Fine-tuning**: Parameter-efficient fine-tuning\n",
        "- **Comprehensive Logging**: Training metrics and progress tracking\n",
        "- **Model Evaluation**: Test and evaluation capabilities\n",
        "\n",
        "## Prerequisites:\n",
        "- TPU v5e-8 instance\n",
        "- JAX, Flax, and Optax installed\n",
        "- Transformers library with TPU support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "ee1d1dde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee1d1dde",
        "lines_to_next_cell": 1,
        "outputId": "ec29d76e-5280-43c2-9d6f-742f043ddb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.5.3\n",
            "JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n",
            "TPU cores available: 1\n",
            "TPU backend: tpu\n",
            "\n",
            "‚úÖ TPU connection successful!\n"
          ]
        }
      ],
      "source": [
        "# Check TPU availability and setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Verify TPU connection\n",
        "try:\n",
        "    import jax\n",
        "    print(f\"JAX version: {jax.__version__}\")\n",
        "    print(f\"JAX devices: {jax.devices()}\")\n",
        "    print(f\"TPU cores available: {jax.device_count()}\")\n",
        "    print(f\"TPU backend: {jax.default_backend()}\")\n",
        "\n",
        "    if jax.default_backend() != 'tpu':\n",
        "        print(\"\\n‚ö†Ô∏è  WARNING: TPU not detected! Current backend:\", jax.default_backend())\n",
        "        print(\"Please ensure you're connected to a TPU instance.\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ TPU connection successful!\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"‚ùå JAX not installed. Installing required packages...\")\n",
        "    print(\"Run: pip install jax[tpu] flax optax -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7b01b2f",
      "metadata": {
        "id": "f7b01b2f"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "57017260",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57017260",
        "outputId": "b3181043-b131-4023-9484-706cae28f960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for TPU\n",
        "!pip install -q jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -q flax optax\n",
        "!pip install -q transformers datasets accelerate peft\n",
        "!pip install -q sentencepiece protobuf\n",
        "!pip install -q tensorboard\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d02c414",
      "metadata": {
        "id": "6d02c414"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "86cdb439",
      "metadata": {
        "id": "86cdb439"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# JAX and Flax for TPU\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Transformers and Datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainerCallback,\n",
        "    TrainerState,\n",
        "    TrainerControl\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Utilities\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(f'tpu_training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea8c29d",
      "metadata": {
        "id": "cea8c29d"
      },
      "source": [
        "## Configuration for TPU v5e-8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009aa88b",
      "metadata": {
        "id": "009aa88b"
      },
      "source": [
        "## Custom Training Callbacks for Detailed Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "a6f73490",
      "metadata": {
        "id": "a6f73490"
      },
      "outputs": [],
      "source": [
        "class DetailedLoggingCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    Custom callback for detailed logging during training on TPU.\n",
        "    Provides comprehensive metrics tracking and progress updates.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_every_n_steps=50):\n",
        "        self.log_every_n_steps = log_every_n_steps\n",
        "        self.start_time = None\n",
        "        self.last_log_time = None\n",
        "        self.step_times = []\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called when training begins\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.last_log_time = self.start_time\n",
        "\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(\"üöÄ TRAINING STARTED ON TPU v5e-8\")\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(f\"Output directory: {args.output_dir}\")\n",
        "        logger.info(f\"Total training steps: {state.max_steps}\")\n",
        "        logger.info(f\"Batch size per device: {args.per_device_train_batch_size}\")\n",
        "        logger.info(f\"Gradient accumulation steps: {args.gradient_accumulation_steps}\")\n",
        "        logger.info(f\"Effective batch size: {args.per_device_train_batch_size * args.gradient_accumulation_steps * args.world_size}\")\n",
        "        logger.info(f\"Number of epochs: {args.num_train_epochs}\")\n",
        "        logger.info(f\"Learning rate: {args.learning_rate}\")\n",
        "        logger.info(f\"Save steps: {args.save_steps}\")\n",
        "        logger.info(f\"Eval steps: {args.eval_steps}\")\n",
        "        logger.info(\"=\"*80)\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called at the end of each training step\"\"\"\n",
        "        if state.global_step % self.log_every_n_steps == 0:\n",
        "            current_time = time.time()\n",
        "            elapsed = current_time - self.start_time\n",
        "            step_elapsed = current_time - self.last_log_time\n",
        "\n",
        "            # Track step times for averaging\n",
        "            self.step_times.append(step_elapsed / self.log_every_n_steps)\n",
        "            if len(self.step_times) > 10:\n",
        "                self.step_times.pop(0)\n",
        "\n",
        "            avg_step_time = sum(self.step_times) / len(self.step_times)\n",
        "\n",
        "            # Calculate ETA\n",
        "            if state.global_step > 0:\n",
        "                remaining_steps = state.max_steps - state.global_step\n",
        "                eta = remaining_steps * avg_step_time\n",
        "                eta_str = f\"{eta / 3600:.1f}h\" if eta > 3600 else f\"{eta / 60:.1f}m\"\n",
        "            else:\n",
        "                eta_str = \"N/A\"\n",
        "\n",
        "            # Get latest metrics safely\n",
        "            loss_str = \"N/A\"\n",
        "            lr_str = \"N/A\"\n",
        "\n",
        "            if state.log_history:\n",
        "                latest_log = state.log_history[-1]\n",
        "                loss = latest_log.get('loss', None)\n",
        "                lr = latest_log.get('learning_rate', None)\n",
        "\n",
        "                if loss is not None and isinstance(loss, (int, float)):\n",
        "                    loss_str = f\"{loss:.4f}\"\n",
        "                if lr is not None and isinstance(lr, (int, float)):\n",
        "                    lr_str = f\"{lr:.2e}\"\n",
        "\n",
        "            # Calculate progress\n",
        "            progress = (state.global_step / state.max_steps) * 100\n",
        "\n",
        "            logger.info(\"‚îÄ\"*80)\n",
        "            logger.info(f\"Step {state.global_step}/{state.max_steps} ({progress:.1f}%)\")\n",
        "            logger.info(f\"Loss: {loss_str}\")\n",
        "            logger.info(f\"Learning Rate: {lr_str}\")\n",
        "            logger.info(f\" Elapsed: {elapsed/60:.1f}m | Step time: {avg_step_time:.2f}s | ETA: {eta_str}\")\n",
        "            logger.info(f\"Epoch: {state.epoch:.2f}\")\n",
        "\n",
        "            self.last_log_time = current_time\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        \"\"\"Called after evaluation\"\"\"\n",
        "        if metrics:\n",
        "            logger.info(\"=\"*80)\n",
        "            logger.info(f\"EVALUATION at Step {state.global_step}\")\n",
        "            logger.info(\"=\"*80)\n",
        "\n",
        "            for key, value in sorted(metrics.items()):\n",
        "                if isinstance(value, (int, float)):\n",
        "                    logger.info(f\"  {key}: {value:.4f}\")\n",
        "                else:\n",
        "                    logger.info(f\"  {key}: {value}\")\n",
        "\n",
        "            logger.info(\"=\"*80)\n",
        "\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called when a checkpoint is saved\"\"\"\n",
        "        logger.info(f\"üíæ Checkpoint saved at step {state.global_step}\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Called when logging occurs\"\"\"\n",
        "        if logs and state.global_step % self.log_every_n_steps == 0:\n",
        "            # Additional metrics logging\n",
        "            if 'grad_norm' in logs:\n",
        "                logger.info(f\"  Gradient norm: {logs['grad_norm']:.4f}\")\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Called when training ends\"\"\"\n",
        "        total_time = time.time() - self.start_time\n",
        "\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(\"TRAINING COMPLETED!\")\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(f\"  Total training time: {total_time/3600:.2f} hours\")\n",
        "        logger.info(f\"Final step: {state.global_step}\")\n",
        "        logger.info(f\"Best metric: {getattr(state, 'best_metric', 'N/A')}\")\n",
        "        logger.info(f\"Best model checkpoint: {getattr(state, 'best_model_checkpoint', 'N/A')}\")\n",
        "        logger.info(\"=\"*80)\n",
        "\n",
        "\n",
        "class EarlyStoppingCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    Early stopping callback to prevent overfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=3, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_metric = None\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        \"\"\"Check if we should stop early\"\"\"\n",
        "        if metrics is None:\n",
        "            return\n",
        "\n",
        "        current_metric = metrics.get('eval_loss')\n",
        "        if current_metric is None:\n",
        "            return\n",
        "\n",
        "        if self.best_metric is None:\n",
        "            self.best_metric = current_metric\n",
        "            logger.info(f\"Initial best metric: {self.best_metric:.4f}\")\n",
        "            return\n",
        "\n",
        "        # Check if improvement\n",
        "        if current_metric < (self.best_metric - self.min_delta):\n",
        "            improvement = self.best_metric - current_metric\n",
        "            self.best_metric = current_metric\n",
        "            self.patience_counter = 0\n",
        "            logger.info(f\"Improved! New best: {self.best_metric:.4f} (‚Üì {improvement:.4f})\")\n",
        "        else:\n",
        "            self.patience_counter += 1\n",
        "            logger.info(f\"  No improvement. Patience: {self.patience_counter}/{self.patience}\")\n",
        "\n",
        "            if self.patience_counter >= self.patience:\n",
        "                logger.info(f\"Early stopping triggered! No improvement for {self.patience} evaluations.\")\n",
        "                control.should_training_stop = True\n",
        "\n",
        "logger.info(\"Custom callbacks defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88ca0133",
      "metadata": {
        "id": "88ca0133"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "6a8136a8",
      "metadata": {
        "id": "6a8136a8"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    logger.info(f\"Random seed set to: {seed}\")\n",
        "\n",
        "def log_system_info():\n",
        "    \"\"\"Log system and TPU information\"\"\"\n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(\"SYSTEM INFORMATION\")\n",
        "    logger.info(\"=\"*80)\n",
        "\n",
        "    # Python version\n",
        "    logger.info(f\"Python version: {sys.version}\")\n",
        "\n",
        "    # PyTorch info\n",
        "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
        "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "    # JAX info\n",
        "    try:\n",
        "        logger.info(f\"JAX version: {jax.__version__}\")\n",
        "        logger.info(f\"JAX backend: {jax.default_backend()}\")\n",
        "        logger.info(f\"JAX devices: {jax.device_count()}\")\n",
        "\n",
        "        if jax.default_backend() == 'tpu':\n",
        "            for i, device in enumerate(jax.devices()):\n",
        "                logger.info(f\"  TPU Device {i}: {device}\")\n",
        "    except:\n",
        "        logger.warning(\"JAX not available or not configured\")\n",
        "\n",
        "    # Disk space\n",
        "    import shutil\n",
        "    total, used, free = shutil.disk_usage(\"/\")\n",
        "    logger.info(f\"Disk space: {free // (2**30)} GB free of {total // (2**30)} GB\")\n",
        "\n",
        "    logger.info(\"=\"*80)\n",
        "\n",
        "def print_model_info(model):\n",
        "    \"\"\"Print detailed model information\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ü§ñ MODEL INFORMATION\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Trainable %: {100 * trainable_params / total_params:.4f}%\")\n",
        "    print(f\"Model dtype: {model.dtype}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Compute perplexity from loss\n",
        "    predictions = np.array(predictions[0]) if isinstance(predictions, tuple) else np.array(predictions)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Calculate accuracy (for tokens)\n",
        "    mask = labels != -100\n",
        "    correct = (predictions.argmax(-1) == labels) & mask\n",
        "    accuracy = correct.sum() / mask.sum() if mask.sum() > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "    }\n",
        "\n",
        "logger.info(\"Utility functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "11cde1d9",
      "metadata": {
        "id": "11cde1d9"
      },
      "outputs": [],
      "source": [
        "# Log system information\n",
        "log_system_info()\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8d274a4",
      "metadata": {
        "id": "b8d274a4"
      },
      "source": [
        "## HuggingFace Authentication (Required for Gemma Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "y4-D5miS1kHQ",
      "metadata": {
        "id": "y4-D5miS1kHQ"
      },
      "outputs": [],
      "source": [
        "!export HF_TOKEN="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "0cdf5818",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cdf5818",
        "outputId": "9fc14359-48e6-479c-888b-a8fe139bd953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è  No token found in environment\n",
            "Enter your HuggingFace token (or press Enter to use interactive login):\n",
            "Token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ HuggingFace authentication successful!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Check if token is in environment\n",
        "hf_token = os.environ.get('HF_TOKEN', None)\n",
        "\n",
        "if hf_token:\n",
        "    print(\"‚úÖ Token found in environment variable\")\n",
        "    login(token=hf_token)\n",
        "    logger.info(\"‚úÖ Logged in to HuggingFace using environment token\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No token found in environment\")\n",
        "    print(\"Enter your HuggingFace token (or press Enter to use interactive login):\")\n",
        "\n",
        "    # Option to enter token manually\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        manual_token = getpass(\"Token: \")\n",
        "        if manual_token:\n",
        "            login(token=manual_token)\n",
        "            logger.info(\"‚úÖ Logged in to HuggingFace using manual token\")\n",
        "        else:\n",
        "            # Interactive login\n",
        "            login()\n",
        "            logger.info(\"‚úÖ Logged in to HuggingFace interactively\")\n",
        "    except:\n",
        "        # Fallback to interactive\n",
        "        login()\n",
        "        logger.info(\"‚úÖ Logged in to HuggingFace interactively\")\n",
        "\n",
        "print(\"‚úÖ HuggingFace authentication successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "adad0b1f",
      "metadata": {
        "id": "adad0b1f"
      },
      "outputs": [],
      "source": [
        "class TPUConfig:\n",
        "    \"\"\"Configuration optimized for TPU v5e-8\"\"\"\n",
        "\n",
        "    # Model settings\n",
        "    model_name = \"google/gemma-3-270m\"\n",
        "    dataset_name = \"pourmand1376/persian-qa-translated\"\n",
        "\n",
        "    # TPU v5e-8 has 8 cores\n",
        "    num_tpu_cores = 8\n",
        "\n",
        "    # Training settings optimized for TPU\n",
        "    # TPU works best with larger batch sizes\n",
        "    per_device_train_batch_size = 4  # Per TPU core\n",
        "    per_device_eval_batch_size = 4\n",
        "    gradient_accumulation_steps = 4  # Effective batch size = 4 * 8 * 4 = 128\n",
        "\n",
        "    # Training parameters\n",
        "    num_train_epochs = 3\n",
        "    learning_rate = 2e-4\n",
        "    warmup_ratio = 0.03\n",
        "    max_grad_norm = 1.0\n",
        "    lr_scheduler_type = \"cosine\"\n",
        "\n",
        "    # Sequence lengths\n",
        "    max_input_length = 512\n",
        "    max_target_length = 64\n",
        "\n",
        "    # LoRA configuration\n",
        "    lora_r = 8\n",
        "    lora_alpha = 16  # Typically 2x lora_r\n",
        "    lora_dropout = 0.05\n",
        "    lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "    # Logging and saving\n",
        "    logging_steps = 50\n",
        "    eval_steps = 200\n",
        "    save_steps = 400\n",
        "    save_total_limit = 3\n",
        "\n",
        "    # Directories\n",
        "    output_dir = \"./outputs_tpu\"\n",
        "    logging_dir = \"./logs_tpu\"\n",
        "    cache_dir = \"./cache\"\n",
        "\n",
        "    # Dataset\n",
        "    val_set_size = 0.1\n",
        "    seed = 42\n",
        "    dataset_size_limit = 100 # Set to an integer to limit training data size\n",
        "\n",
        "config = TPUConfig()\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(config.output_dir, exist_ok=True)\n",
        "os.makedirs(config.logging_dir, exist_ok=True)\n",
        "os.makedirs(config.cache_dir, exist_ok=True)\n",
        "\n",
        "logger.info(\"Configuration initialized for TPU v5e-8\")\n",
        "logger.info(f\"Effective batch size: {config.per_device_train_batch_size * config.num_tpu_cores * config.gradient_accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a42f8e",
      "metadata": {
        "id": "14a42f8e"
      },
      "source": [
        "\n",
        "## Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "4ecd31ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ecd31ca",
        "outputId": "c624f342-964b-4bf1-f4c3-ba6b9d91e2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ü§ñ MODEL INFORMATION\n",
            "================================================================================\n",
            "Total parameters: 268,098,176\n",
            "Trainable parameters: 268,098,176\n",
            "Trainable %: 100.0000%\n",
            "Model dtype: torch.bfloat16\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Loading tokenizer and model...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.model_name,\n",
        "    cache_dir=config.cache_dir,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "logger.info(f\"Tokenizer loaded: {config.model_name}\")\n",
        "logger.info(f\"Vocab size: {len(tokenizer):,}\")\n",
        "logger.info(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "logger.info(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "logger.info(f\"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
        "\n",
        "# Load model - For TPU, we use bfloat16 precision\n",
        "logger.info(\"Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name,\n",
        "    cache_dir=config.cache_dir,\n",
        "    torch_dtype=torch.bfloat16,  # TPU v5e works best with bfloat16\n",
        "    device_map=None,  # We'll handle device placement manually for TPU\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"  # Recommended for Gemma 3\n",
        ")\n",
        "\n",
        "logger.info(f\"Model loaded: {config.model_name}\")\n",
        "\n",
        "# Print model information\n",
        "print_model_info(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "477f2315",
      "metadata": {
        "id": "477f2315"
      },
      "source": [
        "## Apply LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "5411e0f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5411e0f3",
        "outputId": "92a9cfda-e843-466f-f7c7-d100ea87c978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ü§ñ MODEL INFORMATION\n",
            "================================================================================\n",
            "Total parameters: 268,835,456\n",
            "Trainable parameters: 737,280\n",
            "Trainable %: 0.2742%\n",
            "Model dtype: torch.bfloat16\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            " LoRA CONFIGURATION\n",
            "================================================================================\n",
            "Rank (r): 8\n",
            "Alpha: 16\n",
            "Dropout: 0.05\n",
            "Target modules: q_proj, k_proj, v_proj, o_proj\n",
            "Bias: none\n",
            "Task type: CAUSAL_LM\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Applying LoRA configuration...\")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=config.lora_r,\n",
        "    lora_alpha=config.lora_alpha,\n",
        "    lora_dropout=config.lora_dropout,\n",
        "    target_modules=config.lora_target_modules,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "logger.info(\"LoRA applied successfully!\")\n",
        "\n",
        "# Print updated model information\n",
        "print_model_info(model)\n",
        "\n",
        "# Print LoRA configuration details\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" LoRA CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Rank (r): {config.lora_r}\")\n",
        "print(f\"Alpha: {config.lora_alpha}\")\n",
        "print(f\"Dropout: {config.lora_dropout}\")\n",
        "print(f\"Target modules: {', '.join(config.lora_target_modules)}\")\n",
        "print(f\"Bias: none\")\n",
        "print(f\"Task type: CAUSAL_LM\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e8c8b5",
      "metadata": {
        "id": "77e8c8b5"
      },
      "source": [
        "## Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "08803ac0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08803ac0",
        "outputId": "1ce063b8-79a0-4af4-8a97-5e76d94a25a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üìã Sample Dataset Entry:\n",
            "================================================================================\n",
            "input: None\n",
            "instruction: ⁄Ü⁄ØŸàŸÜŸá ŸÖ€å ÿ™ŸàÿßŸÜŸÖ ŸÜŸÇÿßÿ∑ ÿØÿßÿ∫ Ÿà ÿ™ÿßŸàŸÑ Ÿáÿß ÿ±ÿß ÿØÿ±ŸÖÿßŸÜ ⁄©ŸÜŸÖ ŸàŸÇÿ™€å ⁄©Ÿá ŸÖŸàÿ≥⁄©€åŸÜ ŸÜÿØÿßÿ±ŸÖÿü ⁄ÜŸÜÿØ ÿ®ÿßÿ± ÿ®€åÿ±ŸàŸÜ Ÿæ€åÿßÿØŸá ÿ±Ÿà€å ⁄©ÿ±ÿØŸá ÿßŸÖ...\n",
            "original_instruction: How do I treat hot spots and blisters when I have no moleskin?\n",
            "A few times I've been out walking or ...\n",
            "original_output: The key is reducing friction. Duct tape can be a good preventative as long as you get it on before a...\n",
            "output: ⁄©ŸÑ€åÿØ ⁄©ÿßŸáÿ¥ ÿßÿµÿ∑⁄©ÿß⁄© ÿßÿ≥ÿ™. ŸÜŸàÿßÿ± ⁄Üÿ≥ÿ® ŸÖ€å ÿ™ŸàÿßŸÜÿØ €å⁄© Ÿæ€åÿ¥⁄Ø€åÿ±€å ÿÆŸàÿ® ÿ®ÿßÿ¥ÿØ ÿ™ÿß ÿ≤ŸÖÿßŸÜ€å ⁄©Ÿá ÿ¥ŸÖÿß ÿ¢ŸÜ ÿ±ÿß ŸÇÿ®ŸÑ ÿßÿ≤ ÿß€åÿ¨ÿßÿØ ÿ™ÿßŸàŸÑ ...\n",
            "source: stackexchange-outdoors\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "üìù Example Formatted Persian Text:\n",
            "================================================================================\n",
            "### ÿØÿ≥ÿ™Ÿàÿ±:\n",
            "There are twice as many centipedes as humans on a certain island and half as many sheep as humans. How many sheep and humans in total are on the island if the number of centipedes is 100?\n",
            "Give me a detailed solution.\n",
            "\n",
            "### Ÿæÿßÿ≥ÿÆ:\n",
            "The number of humans on the island is 100 / 2 = 50.\n",
            "There are 1/2 * 50 = 25 sheep on the island.\n",
            "The total number of sheep and humans is 25 + 50 = 75.<eos>\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Loading dataset...\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\n",
        "    config.dataset_name,\n",
        "    cache_dir=config.cache_dir\n",
        ")\n",
        "\n",
        "logger.info(f\" Dataset loaded: {config.dataset_name}\")\n",
        "logger.info(f\"Dataset keys: {list(dataset.keys())}\")\n",
        "\n",
        "# Check if 'train' key exists, if not use the first available key\n",
        "if 'train' not in dataset:\n",
        "    dataset_key = list(dataset.keys())[0]\n",
        "    logger.info(f\"'train' key not found, using '{dataset_key}' instead\")\n",
        "    dataset = {'train': dataset[dataset_key]}\n",
        "\n",
        "logger.info(f\"Train samples: {len(dataset['train']):,}\")\n",
        "\n",
        "# Log available columns\n",
        "logger.info(f\"Available columns: {dataset['train'].column_names}\")\n",
        "\n",
        "# Show sample to understand structure\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã Sample Dataset Entry:\")\n",
        "print(\"=\"*80)\n",
        "sample = dataset['train'][0]\n",
        "for key, value in sample.items():\n",
        "    display_value = str(value)[:100] + \"...\" if len(str(value)) > 100 else str(value)\n",
        "    print(f\"{key}: {display_value}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Filter out samples that are too long or have missing fields\n",
        "def filter_examples(example):\n",
        "    \"\"\"Filter out invalid examples - using Persian columns\"\"\"\n",
        "    # For Persian QA dataset, use original_instruction and original_output\n",
        "    instruction = example.get('original_instruction', '').strip()\n",
        "    output = example.get('original_output', '').strip()\n",
        "\n",
        "    # Must have both instruction and output in Persian\n",
        "    return len(instruction) > 0 and len(output) > 0\n",
        "\n",
        "logger.info(\"Filtering invalid examples...\")\n",
        "initial_count = len(dataset['train'])\n",
        "dataset = dataset.filter(filter_examples)\n",
        "filtered_count = len(dataset['train'])\n",
        "logger.info(f\"After filtering - Train samples: {filtered_count:,} (removed {initial_count - filtered_count:,})\")\n",
        "\n",
        "if filtered_count == 0:\n",
        "    logger.error(\"  Dataset is empty after filtering!\")\n",
        "    raise ValueError(\"Dataset is empty. Please check the dataset structure and column names.\")\n",
        "\n",
        "\n",
        "\n",
        "# Format the dataset for causal language modeling using Persian columns\n",
        "def format_instruction(example):\n",
        "    \"\"\"Format examples as instruction-following prompts using Persian text\"\"\"\n",
        "    # Use Persian columns\n",
        "    instruction = example.get('original_instruction', '').strip()\n",
        "    output = example.get('original_output', '').strip()\n",
        "\n",
        "    # Format as instruction-following\n",
        "    text = f\"### ÿØÿ≥ÿ™Ÿàÿ±:\\n{instruction}\\n\\n### Ÿæÿßÿ≥ÿÆ:\\n{output}{tokenizer.eos_token}\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "logger.info(\"Formatting dataset with Persian columns...\")\n",
        "dataset = dataset.map(\n",
        "    format_instruction,\n",
        "    remove_columns=dataset['train'].column_names,\n",
        "    desc=\"Formatting Persian instructions\"\n",
        ")\n",
        "\n",
        "# Apply dataset size limiting if specified\n",
        "if config.dataset_size_limit is not None and isinstance(config.dataset_size_limit, int) and config.dataset_size_limit > 0:\n",
        "    if config.dataset_size_limit >= len(dataset['train']):\n",
        "        logger.info(f\"Dataset size limit ({config.dataset_size_limit}) >= total dataset size ({len(dataset['train'])}), using full dataset.\")\n",
        "    else:\n",
        "        logger.info(f\"Limiting dataset to {config.dataset_size_limit} samples (shuffling first)...\")\n",
        "        # Shuffle the dataset before taking the subset\n",
        "        dataset['train'] = dataset['train'].shuffle(seed=config.seed)\n",
        "        dataset['train'] = dataset['train'].select(range(config.dataset_size_limit))\n",
        "        logger.info(f\"Dataset limited to {len(dataset['train'])} samples.\")\n",
        "\n",
        "\n",
        "# Split into train and validation\n",
        "logger.info(\"Splitting dataset...\")\n",
        "dataset = dataset['train'].train_test_split(\n",
        "    test_size=config.val_set_size,\n",
        "    seed=config.seed\n",
        ")\n",
        "\n",
        "logger.info(f\" Final dataset split:\")\n",
        "logger.info(f\"  Train: {len(dataset['train']):,}\")\n",
        "logger.info(f\"  Validation: {len(dataset['test']):,}\")\n",
        "\n",
        "# Show formatted example\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù Example Formatted Persian Text:\")\n",
        "print(\"=\"*80)\n",
        "print(dataset['train'][0]['text'][:500])\n",
        "if len(dataset['train'][0]['text']) > 500:\n",
        "    print(\"... (truncated)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5b97813",
      "metadata": {
        "id": "d5b97813"
      },
      "source": [
        "## Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "62f42dbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2476518329044534b443047e3bf58988",
            "1cb2b9b4cd1e42c889aab272d8994575",
            "5b7aacd5a3de4995947bf147d1c1e85e",
            "ef6395ea6cb340d8a2d31072d01f6f79",
            "8e4a55371499459585f54d8abc7f34bf",
            "392adac785f74f6d83dd2bde98a970eb",
            "a5f6b6609f534e5998fc3a01701bbf1c",
            "1734bb424e6a44ec857531c6c01b1458",
            "e5215f51ba64406e94f1a12f6fcd330f",
            "0f304803f9f445f9b69c762b79223b70",
            "84acf7ce29534ef0bfc77041fc90376f"
          ]
        },
        "id": "62f42dbf",
        "outputId": "3c5bc44f-fcd3-43dd-ce68-7622a68789c0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2476518329044534b443047e3bf58988"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "logger.info(\"Tokenizing dataset...\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the texts\"\"\"\n",
        "    outputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=config.max_input_length,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
        "    return outputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    desc=\"Tokenizing dataset\",\n",
        ")\n",
        "\n",
        "logger.info(\"Tokenization complete!\")\n",
        "logger.info(f\"Train samples: {len(tokenized_dataset['train']):,}\")\n",
        "logger.info(f\"Validation samples: {len(tokenized_dataset['test']):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d7016a2",
      "metadata": {
        "id": "6d7016a2"
      },
      "source": [
        "## Setup Training Arguments for TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "c0b5abf2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b5abf2",
        "outputId": "43a7dc40-cc7d-4b63-9390-1ec8d7555af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Training Configuration:\n",
            "==================================================\n",
            "TPU cores: 8\n",
            "Per device batch size: 4\n",
            "Gradient accumulation: 4\n",
            "Effective batch size: 128\n",
            "Learning rate: 0.0002\n",
            "Epochs: 3\n",
            "Precision: bfloat16\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Setting up training arguments for TPU...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # Output and logging\n",
        "    output_dir=config.output_dir,\n",
        "    logging_dir=config.logging_dir,\n",
        "    logging_steps=config.logging_steps,\n",
        "\n",
        "    # Training parameters\n",
        "    num_train_epochs=config.num_train_epochs,\n",
        "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "\n",
        "    # Optimization\n",
        "    learning_rate=config.learning_rate,\n",
        "    warmup_ratio=config.warmup_ratio,\n",
        "    max_grad_norm=config.max_grad_norm,\n",
        "    lr_scheduler_type=config.lr_scheduler_type,\n",
        "    optim=\"adafactor\",  # Adafactor works well on TPU\n",
        "\n",
        "    # Precision - TPU v5e supports bfloat16\n",
        "    bf16=True,  # Use bfloat16 for TPU\n",
        "    fp16=False,\n",
        "\n",
        "    # Evaluation\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=config.eval_steps,\n",
        "\n",
        "    # Saving\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=config.save_steps,\n",
        "    save_total_limit=config.save_total_limit,\n",
        "\n",
        "    # Performance\n",
        "    dataloader_num_workers=0,  # TPU doesn't need multiple workers\n",
        "    group_by_length=False,  # Can cause issues on TPU\n",
        "    dataloader_pin_memory=False,\n",
        "\n",
        "    # Reporting\n",
        "    report_to=\"tensorboard\",\n",
        "\n",
        "    # TPU specific\n",
        "    tpu_num_cores=config.num_tpu_cores,\n",
        "    dataloader_drop_last=True,  # Recommended for TPU\n",
        "\n",
        "    # Misc\n",
        "    seed=config.seed,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "logger.info(\"Training arguments configured!\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Training Configuration:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"TPU cores: {config.num_tpu_cores}\")\n",
        "print(f\"Per device batch size: {config.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {config.per_device_train_batch_size * config.num_tpu_cores * config.gradient_accumulation_steps}\")\n",
        "print(f\"Learning rate: {config.learning_rate}\")\n",
        "print(f\"Epochs: {config.num_train_epochs}\")\n",
        "print(f\"Precision: bfloat16\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e584c8f",
      "metadata": {
        "id": "3e584c8f"
      },
      "source": [
        "## Initialize Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "7527f07a",
      "metadata": {
        "id": "7527f07a"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # We're doing causal language modeling, not masked LM\n",
        ")\n",
        "\n",
        "logger.info(\"Data collator initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f77323e9",
      "metadata": {
        "id": "f77323e9"
      },
      "source": [
        "## Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "dcfd3c6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcfd3c6d",
        "outputId": "03273cca-da52-48dd-d411-792ad90096bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "  Training Information:\n",
            "================================================================================\n",
            "Steps per epoch: 5\n",
            "Total training steps: 15\n",
            "Evaluation every: 200 steps\n",
            "Save checkpoint every: 400 steps\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Initializing Trainer with custom callbacks...\")\n",
        "\n",
        "# Initialize callbacks\n",
        "detailed_logger = DetailedLoggingCallback(log_every_n_steps=config.logging_steps)\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    patience=3,\n",
        "    min_delta=0.001\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[detailed_logger, early_stopping],\n",
        ")\n",
        "\n",
        "logger.info(\"  Trainer initialized successfully!\")\n",
        "\n",
        "# Calculate and display training info\n",
        "steps_per_epoch = len(trainer.get_train_dataloader()) // config.gradient_accumulation_steps\n",
        "total_steps = steps_per_epoch * config.num_train_epochs\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"  Training Information:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "print(f\"Evaluation every: {config.eval_steps} steps\")\n",
        "print(f\"Save checkpoint every: {config.save_steps} steps\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34431b46",
      "metadata": {
        "id": "34431b46"
      },
      "source": [
        "## Start Training on TPU\n",
        "\n",
        "**‚ö†Ô∏è Important**: Before running training, ensure:\n",
        "1. You're connected to the TPU v5e-8 instance\n",
        "2. All previous cells have been executed successfully\n",
        "3. You have sufficient disk space for checkpoints\n",
        "\n",
        "This will take several hours depending on the dataset size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "a5fdcd73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "a5fdcd73",
        "outputId": "e1c60701-10bd-4828-ac44-363a0aec460e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18/18 00:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Training Results:\n",
            "==================================================\n",
            "train_runtime: 30.0221\n",
            "train_samples_per_second: 8.794\n",
            "train_steps_per_second: 0.6\n",
            "total_flos: 81963141562368.0\n",
            "train_loss: 3.3194444444444446\n",
            "epoch: 3.0\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"=\"*70)\n",
        "logger.info(\"STARTING TRAINING ON TPU\")\n",
        "logger.info(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    # Start training\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    logger.info(\"=\"*70)\n",
        "    logger.info(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    logger.info(\"=\"*70)\n",
        "\n",
        "    # Print training results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training Results:\")\n",
        "    print(\"=\"*50)\n",
        "    for key, value in train_result.metrics.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Save final model\n",
        "    final_model_path = f\"{config.output_dir}/final_model\"\n",
        "    trainer.save_model(final_model_path)\n",
        "    logger.info(f\"Final model saved to: {final_model_path}\")\n",
        "\n",
        "    # Save tokenizer\n",
        "    tokenizer.save_pretrained(final_model_path)\n",
        "    logger.info(f\"Tokenizer saved to: {final_model_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Training failed with error: {str(e)}\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fb1d33",
      "metadata": {
        "id": "b1fb1d33"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "974aeb85",
      "metadata": {
        "id": "974aeb85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "275c6873-f318-4927-c0ff-312266361285"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Final Evaluation Results:\n",
            "==================================================\n",
            "eval_loss: 3.1328\n",
            "eval_runtime: 0.2648\n",
            "eval_samples_per_second: 30.2130\n",
            "eval_steps_per_second: 7.5530\n",
            "epoch: 3.0000\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1203"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# ## Evaluate Model\n",
        "logger.info(\"Running final evaluation on the trained model...\")\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Final Evaluation Results:\")\n",
        "print(\"=\"*50)\n",
        "for key, value in eval_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n",
        "# ## Merge LoRA Adapters and Save Standalone Model\n",
        "logger.info(\"Merging LoRA adapters into the base model to create a standalone model...\")\n",
        "\n",
        "# The trainer holds the PeftModel, merge and unload it\n",
        "merged_model = trainer.model.merge_and_unload()\n",
        "logger.info(\"Model merged successfully.\")\n",
        "\n",
        "# --- FIX START ---\n",
        "# Move the merged model from the TPU to the CPU before saving.\n",
        "logger.info(\"Moving merged model to CPU for saving...\")\n",
        "merged_model.to('cpu')\n",
        "logger.info(\"Model is now on CPU.\")\n",
        "# --- FIX END ---\n",
        "\n",
        "# Define a new path for the final merged model\n",
        "final_model_path = f\"{config.output_dir}/final_merged_model\"\n",
        "\n",
        "# Save the merged model (now on CPU) and the tokenizer\n",
        "merged_model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "logger.info(f\"Final standalone model saved successfully to: {final_model_path}\")\n",
        "\n",
        "# IMPORTANT: Clean up memory to make space for the reloaded model\n",
        "import gc\n",
        "del model\n",
        "del trainer\n",
        "del merged_model\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f58b438",
      "metadata": {
        "id": "2f58b438"
      },
      "source": [
        "## Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch_xla.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "HKnH15n3g185",
        "outputId": "2a794666-1276-4ddb-c1d4-e7fe49f5087b"
      },
      "id": "HKnH15n3g185",
      "execution_count": 64,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2221395553.py:20: DeprecationWarning: Use torch_xla.device instead\n",
            "  device = xm.xla_device()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Testing Model Inference:\n",
            "======================================================================\n",
            "\n",
            "[Test 1]\n",
            "ÿ≥ŸàÿßŸÑ: Ÿæÿß€åÿ™ÿÆÿ™ ÿß€åÿ±ÿßŸÜ ⁄©ÿ¨ÿßÿ≥ÿ™ÿü\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2221395553.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[Test {i}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ÿ≥ŸàÿßŸÑ: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreloaded_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ÿæÿßÿ≥ÿÆ: {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2221395553.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(model, tokenizer, question, max_length)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBEAM_SAMPLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBEAM_SEARCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m             \u001b[0;31m# 11. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2552\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3400\u001b[0m             \u001b[0;31m# c. Retrieve top-K continuations, i.e. select the next token (greedy or sampling) and then keep the best\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3401\u001b[0m             \u001b[0;31m# continuations among all beams based on the accumulated scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3402\u001b[0;31m             topk_log_probs, topk_running_sequences, topk_running_beam_indices = self._get_top_k_continuations(\n\u001b[0m\u001b[1;32m   3403\u001b[0m                 \u001b[0maccumulated_log_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m                 \u001b[0mrunning_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrunning_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_get_top_k_continuations\u001b[0;34m(self, accumulated_log_probs, running_sequences, running_beam_indices, cur_len, decoder_prompt_len, do_sample, beams_to_keep, num_beams, vocab_size, batch_size)\u001b[0m\n\u001b[1;32m   3092\u001b[0m         \u001b[0;31m# Gather the top K scores from _all_ beams.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3094\u001b[0;31m             topk_indices = torch.multinomial(\n\u001b[0m\u001b[1;32m   3095\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulated_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeams_to_keep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m             )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0147c48",
      "metadata": {
        "id": "e0147c48"
      },
      "source": [
        "## Save and Export Model for Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd5a009",
      "metadata": {
        "id": "afd5a009"
      },
      "outputs": [],
      "source": [
        "# Save LoRA adapters separately (smaller file size)\n",
        "lora_output_dir = f\"{config.output_dir}/lora_adapters\"\n",
        "os.makedirs(lora_output_dir, exist_ok=True)\n",
        "\n",
        "logger.info(\"Reloading model with LoRA adapters for saving...\")\n",
        "# Reload the base model and apply LoRA config\n",
        "from peft import PeftModel, LoraConfig, TaskType\n",
        "\n",
        "# Reload the base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name,\n",
        "    torch_dtype=torch.bfloat16,  # Ensure bfloat16\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\", # Match training config\n",
        ")\n",
        "\n",
        "# Define the same LoRA config used for training\n",
        "lora_config = LoraConfig(\n",
        "    r=config.lora_r,\n",
        "    lora_alpha=config.lora_alpha,\n",
        "    lora_dropout=config.lora_dropout,\n",
        "    target_modules=config.lora_target_modules,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# Load the trained LoRA adapters onto the base model\n",
        "# Assuming the trainer saved the LoRA adapters implicitly to config.output_dir\n",
        "# We need to point PeftModel.from_pretrained to the correct checkpoint directory\n",
        "# Let's assume the latest checkpoint in config.output_dir contains the adapters\n",
        "# Find the latest checkpoint directory\n",
        "latest_checkpoint = trainer.state.best_model_checkpoint # Use the best checkpoint path from the trainer state\n",
        "\n",
        "if latest_checkpoint:\n",
        "    logger.info(f\"Loading LoRA adapters from: {latest_checkpoint}\")\n",
        "    model_with_lora = PeftModel.from_pretrained(base_model, latest_checkpoint)\n",
        "    logger.info(\"Model with LoRA adapters reloaded.\")\n",
        "\n",
        "    # Save the LoRA adapters separately\n",
        "    model_with_lora.save_pretrained(lora_output_dir)\n",
        "    tokenizer.save_pretrained(lora_output_dir) # Save tokenizer as well\n",
        "\n",
        "    logger.info(f\"LoRA adapters saved to: {lora_output_dir}\")\n",
        "\n",
        "    # Print file sizes\n",
        "    import glob\n",
        "    for file_path in glob.glob(f\"{lora_output_dir}/*\"):\n",
        "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "        print(f\"{os.path.basename(file_path)}: {size_mb:.2f} MB\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Model Training and Evaluation Complete!\")\n",
        "    print(\"=\"*70)\n",
        "    # Use final_model_path which was set to save the merged model\n",
        "    print(f\"Final merged model: {final_model_path}\")\n",
        "    print(f\"LoRA adapters: {lora_output_dir}\")\n",
        "    print(f\"Logs: {config.logging_dir}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "else:\n",
        "    logger.error(\"Could not find the latest checkpoint to load LoRA adapters from.\")\n",
        "    print(\"Error: Could not find the latest checkpoint to load LoRA adapters from.\")\n",
        "\n",
        "# IMPORTANT: Clean up memory\n",
        "import gc\n",
        "del base_model\n",
        "if 'model_with_lora' in locals():\n",
        "    del model_with_lora\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ec1239",
      "metadata": {
        "id": "f1ec1239"
      },
      "source": [
        "## TPU Connection Verification (Run First)\n",
        "\n",
        "Before starting the full training, run this cell to verify your TPU connection."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/MyDrive\")"
      ],
      "metadata": {
        "id": "8AQ9qgCVkD0B"
      },
      "id": "8AQ9qgCVkD0B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"## GPU Inference Testing\n",
        "\n",
        "This section loads the saved model and runs inference on GPU instead of TPU.\n",
        "Run this in a separate session or after disconnecting from TPU.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration\n",
        "model_path = \"./outputs_tpu/final_merged_model\"  # Update this path if needed\n",
        "\n",
        "# Check GPU availability\n",
        "logger.info(\"Checking GPU availability...\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    logger.info(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    logger.warning(\"‚ö†Ô∏è No GPU detected, using CPU\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ Loading Model for GPU Inference\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load tokenizer\n",
        "logger.info(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_path,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "logger.info(f\"‚úÖ Tokenizer loaded from: {model_path}\")\n",
        "\n",
        "# Load model for GPU\n",
        "logger.info(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"  # Consistent with training\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "logger.info(f\"‚úÖ Model loaded and placed on: {model.device}\")\n",
        "\n",
        "# Print model info\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "def generate_response(question, max_length=256, temperature=0.7, top_p=0.9, num_beams=4):\n",
        "    \"\"\"\n",
        "    Generate response for a given Persian question on GPU.\n",
        "\n",
        "    Args:\n",
        "        question: Input question in Persian\n",
        "        max_length: Maximum length of generated response\n",
        "        temperature: Sampling temperature (higher = more creative)\n",
        "        top_p: Nucleus sampling parameter\n",
        "        num_beams: Number of beams for beam search\n",
        "\n",
        "    Returns:\n",
        "        Generated answer string\n",
        "    \"\"\"\n",
        "    prompt = f\"### ÿ≥ŸàÿßŸÑ:\\n{question}\\n\\n### Ÿæÿßÿ≥ÿÆ:\\n\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Move inputs to model's device\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_beams=num_beams,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.2,  # Reduce repetition\n",
        "            no_repeat_ngram_size=3,  # Prevent 3-gram repetition\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract answer part\n",
        "    if \"### Ÿæÿßÿ≥ÿÆ:\" in response:\n",
        "        answer = response.split(\"### Ÿæÿßÿ≥ÿÆ:\")[-1].strip()\n",
        "    else:\n",
        "        answer = response.strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "# Test questions\n",
        "test_questions = [\n",
        "    \"Ÿæÿß€åÿ™ÿÆÿ™ ÿß€åÿ±ÿßŸÜ ⁄©ÿ¨ÿßÿ≥ÿ™ÿü\",\n",
        "    \"⁄Ü⁄ØŸàŸÜŸá ŸÖ€å‚Äåÿ™ŸàÿßŸÜ €å⁄© ŸÖÿØŸÑ ÿ≤ÿ®ÿßŸÜ€å ÿ±ÿß ÿ¢ŸÖŸàÿ≤ÿ¥ ÿØÿßÿØÿü\",\n",
        "    \"ÿ™ŸÅÿßŸàÿ™ ÿ®€åŸÜ ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å Ÿà €åÿßÿØ⁄Ø€åÿ±€å ŸÖÿßÿ¥€åŸÜ ⁄Ü€åÿ≥ÿ™ÿü\",\n",
        "    \"ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±€åŸÜ ÿ≥€åÿßÿ±Ÿá ŸÖŸÜÿ∏ŸàŸÖŸá ÿ¥ŸÖÿ≥€å ⁄©ÿØÿßŸÖ ÿßÿ≥ÿ™ÿü\",\n",
        "    \"ÿ≤ÿ®ÿßŸÜ ÿ®ÿ±ŸÜÿßŸÖŸá‚ÄåŸÜŸà€åÿ≥€å Ÿæÿß€åÿ™ŸàŸÜ ⁄ÜŸá ŸÖÿ≤ÿß€åÿß€å€å ÿØÿßÿ±ÿØÿü\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üß™ Testing Model Inference on GPU\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Track inference time\n",
        "import time\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n[Test {i}/{len(test_questions)}]\")\n",
        "    print(f\"ÿ≥ŸàÿßŸÑ: {question}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    answer = generate_response(question)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Ÿæÿßÿ≥ÿÆ: {answer}\")\n",
        "    print(f\"‚è±Ô∏è  Inference time: {inference_time:.2f}s\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "logger.info(\"‚úÖ GPU Inference testing complete!\")"
      ],
      "metadata": {
        "id": "V_jynAkmkVNT"
      },
      "id": "V_jynAkmkVNT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Interactive testing function\n",
        "def interactive_test():\n",
        "    \"\"\"\n",
        "    Interactive testing mode - allows users to input their own questions.\n",
        "    Type 'exit' or 'quit' to stop.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéØ Interactive Testing Mode\")\n",
        "    print(\"Enter your questions in Persian (or type 'exit' to quit)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            question = input(\"\\nüí¨ ÿ≥ŸàÿßŸÑ ÿ¥ŸÖÿß: \").strip()\n",
        "\n",
        "            if question.lower() in ['exit', 'quit', 'ÿÆÿ±Ÿàÿ¨']:\n",
        "                print(\"üëã Exiting interactive mode...\")\n",
        "                break\n",
        "\n",
        "            if not question:\n",
        "                print(\"‚ö†Ô∏è  Please enter a question!\")\n",
        "                continue\n",
        "\n",
        "            start_time = time.time()\n",
        "            answer = generate_response(question)\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            print(f\"\\nü§ñ Ÿæÿßÿ≥ÿÆ: {answer}\")\n",
        "            print(f\"‚è±Ô∏è  Time: {inference_time:.2f}s\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüëã Exiting interactive mode...\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)}\")\n",
        "\n",
        "\n",
        "# Uncomment the line below to enable interactive testing\n",
        "# interactive_test()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ All tests completed!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model location: {model_path}\")\n",
        "print(f\"Device: {model.device}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "A_BvHPV0lSsR"
      },
      "id": "A_BvHPV0lSsR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2476518329044534b443047e3bf58988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cb2b9b4cd1e42c889aab272d8994575",
              "IPY_MODEL_5b7aacd5a3de4995947bf147d1c1e85e",
              "IPY_MODEL_ef6395ea6cb340d8a2d31072d01f6f79"
            ],
            "layout": "IPY_MODEL_8e4a55371499459585f54d8abc7f34bf"
          }
        },
        "1cb2b9b4cd1e42c889aab272d8994575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392adac785f74f6d83dd2bde98a970eb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a5f6b6609f534e5998fc3a01701bbf1c",
            "value": "Tokenizing‚Äádataset:‚Äá100%"
          }
        },
        "5b7aacd5a3de4995947bf147d1c1e85e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1734bb424e6a44ec857531c6c01b1458",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5215f51ba64406e94f1a12f6fcd330f",
            "value": 10
          }
        },
        "ef6395ea6cb340d8a2d31072d01f6f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f304803f9f445f9b69c762b79223b70",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_84acf7ce29534ef0bfc77041fc90376f",
            "value": "‚Äá10/10‚Äá[00:00&lt;00:00,‚Äá693.02‚Äáexamples/s]"
          }
        },
        "8e4a55371499459585f54d8abc7f34bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "392adac785f74f6d83dd2bde98a970eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5f6b6609f534e5998fc3a01701bbf1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1734bb424e6a44ec857531c6c01b1458": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5215f51ba64406e94f1a12f6fcd330f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f304803f9f445f9b69c762b79223b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84acf7ce29534ef0bfc77041fc90376f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}